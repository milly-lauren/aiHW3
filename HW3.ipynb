{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/milly-lauren/aiHW3/blob/code/HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLnerd9qNyZ9",
        "colab_type": "text"
      },
      "source": [
        "Homework 3:\n",
        "Changed the required path and tried making batch size smaller and divided the steps per epoch to make them smaller"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4baUV0fwT-j",
        "colab_type": "text"
      },
      "source": [
        "# Test the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03IyOYlyxTtg",
        "colab_type": "code",
        "outputId": "a633d1de-d38b-46d4-d86c-794f88d182de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKanl_K40R8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -q \"/content/drive/My Drive/RuTanks7000_v1/RuTanks7000_v1.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxS5QwkqNu6C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports\n",
        "import os\n",
        "from keras.applications import NASNetLarge\n",
        "from keras import models, layers, optimizers, backend\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "\n",
        "def main():\n",
        "    start = timer()\n",
        "    level1()  # Training the classifier only\n",
        "    level2()  # Training the pretrained model + the trained classifier from level 1\n",
        "    end = timer()\n",
        "    print(\"Time elapsed in minutes: \", ((end - start)/60))\n",
        "\n",
        "# Setting Parameters ##################################################################\n",
        "# image settings\n",
        "img_height,img_width = 331, 331  # For NASNetLarge\n",
        "\n",
        "# classes\n",
        "classnames = [\"Background\", \"BMP2\", \"Buk-M1-2\", \"T14\", \"T90\", \"ZSU23\"]\n",
        "classes = len(classnames)\n",
        "\n",
        "# path settings\n",
        "#path = 'C:\\\\Users\\\\uguru\\\\Documents\\\\ML4MIL\\\\'\n",
        "path = '/content/drive/My Drive/'\n",
        "dataset_path = 'RuTanks7000_v1/'\n",
        "weights_path = path+'trained_models/trained_weights/weights_temp.h5'\n",
        "model_path = path+'trained_models/RuTanks7000_v1_'\n",
        "TensorBoardLogDir = path+'logs'\n",
        "\n",
        "nbrTrainImages = 7000  # per class\n",
        "nbrTestImages = 0  # Value gets accurate after counting (Total Number of test images)\n",
        "for ImagesClass in os.listdir(dataset_path+'test/'):\n",
        "    nbrTestImages += len(os.listdir(dataset_path+'test/'+ImagesClass))\n",
        "\n",
        "# unfreezing the base network up to a specific layer in Level2:\n",
        "freezeUptoLayer = \"normal_add_1_15\"   # NASNetLarge\n",
        "\n",
        "# hyperparameters\n",
        "learning_rate = 0.0002  # Learning_rate in Level 2 = learning_rate/10\n",
        "lr_decay = 0.0001\n",
        "batch = 8\n",
        "fcLayer1 = 32\n",
        "dropout = 0.5\n",
        "\n",
        "epochsL1 = 3\n",
        "patiencel1 = 1\n",
        "factorL1 = 0.5\n",
        "\n",
        "epochsL2 = 3\n",
        "patiencel2 = 1\n",
        "factorL2 = 0.5\n",
        "\n",
        "verbose_train = 1\n",
        "\n",
        "# datagenerators https://keras.io/preprocessing/image/\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    fill_mode='nearest',\n",
        "    horizontal_flip=True)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    dataset_path+'train',\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch,\n",
        "    shuffle=True,\n",
        "    classes=classnames,\n",
        "    class_mode='categorical')\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    dataset_path+'test',\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=1,\n",
        "    shuffle=True,\n",
        "    classes=classnames,\n",
        "    class_mode='categorical')\n",
        "#########################################################################################\n",
        "\n",
        "# function to plot results of model performance\n",
        "def plot(h,t,e):\n",
        "    history_dict = h[0]\n",
        "    loss_values = history_dict['loss']\n",
        "    validation_loss_values = history_dict['val_loss']\n",
        "    acc_values = history_dict['acc']\n",
        "    validation_acc_values = history_dict['val_acc']\n",
        "    epochs_range = range(1, e + 1)\n",
        "\n",
        "    # Plotting Training and Validation loss of the corresponding Model\n",
        "    plt.plot(epochs_range, loss_values, 'bo', label='Training loss')\n",
        "    plt.plot(epochs_range, validation_loss_values, 'ro', label='Validation loss')\n",
        "    plt.title('Training and validation loss of ' + t)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.yticks(np.arange(0, 3.1, step=0.2))\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Plotting Training and Validation accuracy of the corresponding Model\n",
        "    plt.plot(epochs_range, acc_values, 'bo', label='Training accuracy')\n",
        "    plt.plot(epochs_range, validation_acc_values, 'ro', label='Validation accuracy')\n",
        "    plt.title('Training and validation accuracy of ' + t)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.yticks(np.arange(0.3, 1.1, step=0.1))\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# LEVEL1 - Training of densely connected layers\n",
        "def level1():\n",
        "    # Building the model using the pretrained model\n",
        "    conv_base1 = NASNetLarge(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
        "    print(\"\\n### LEVEL1 ###\\npretrained network:\")\n",
        "    conv_base1.summary()\n",
        "    model = models.Sequential()\n",
        "    model.add(conv_base1)\n",
        "    model.add(layers.GlobalAveragePooling2D())\n",
        "    model.add(layers.Dense(fcLayer1, activation='relu'))\n",
        "    model.add(layers.Dropout(dropout))\n",
        "    model.add(layers.Dense(classes, activation='softmax'))\n",
        "\n",
        "    # freezing the base network\n",
        "    print(\"trainable layers bevor freezing:\", int(len(model.trainable_weights)/2)) # weights = weights + bias = 2 pro layer\n",
        "    conv_base1.trainable = False\n",
        "    print(\"trainable layers after freezing:\", int(len(model.trainable_weights)/2))\n",
        "    print(\"\\npretrained network + densely connected classifier\")\n",
        "    model.summary()\n",
        "\n",
        "    # training the added layers only\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=learning_rate, decay=lr_decay), metrics=['acc'])\n",
        "\n",
        "    callbacks_list_L1 = [ModelCheckpoint(filepath=weights_path, save_weights_only=True, monitor='val_acc', verbose=1, save_best_only=True),\n",
        "                      ReduceLROnPlateau(monitor='val_acc', factor=factorL1, patience=patiencel1, verbose=1),\n",
        "                      TensorBoard(log_dir=TensorBoardLogDir+'\\\\level1')]\n",
        "\n",
        "    print(\"\\n### Level1 Training ... \")\n",
        "    # training the model\n",
        "    history = model.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=(nbrTrainImages * classes) // 100 * batch,\n",
        "        epochs=epochsL1,\n",
        "        callbacks=callbacks_list_L1,\n",
        "        validation_data=test_generator,\n",
        "        validation_steps=nbrTestImages,\n",
        "        verbose=verbose_train)\n",
        "\n",
        "    history_val1 = [history.history]  # saving all results of the final test\n",
        "    plot(history_val1, \"LEVEL1:\", epochsL1)\n",
        "    print(\"\\n### LEVEL1 Training finished successfully ###\")\n",
        "\n",
        "    print(\"\\nLoading trained weights from \" + weights_path + \" ...\")\n",
        "    model.load_weights(weights_path)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=learning_rate), metrics=['acc'])\n",
        "    print(\"\\n### Saving Level1 Model to \", model_path+'l1.h5', \" ... \")\n",
        "    model.save(model_path+'l1.h5')\n",
        "\n",
        "\n",
        "# LEVEL2 - Training pretrained network and trained densely connected layers\n",
        "def level2():\n",
        "    # Destroying the current TF graph - https://keras.io/backend/\n",
        "    backend.clear_session()\n",
        "    print(\"\\n### LEVEL2 ###\")\n",
        "    conv_base2 = NASNetLarge(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
        "    model2 = models.Sequential()\n",
        "    model2.add(conv_base2)\n",
        "    model2.add(layers.GlobalAveragePooling2D())\n",
        "    model2.add(layers.Dense(fcLayer1, activation='relu'))\n",
        "    model2.add(layers.Dropout(dropout))\n",
        "    model2.add(layers.Dense(classes, activation='softmax'))\n",
        "\n",
        "    print(\"\\nLoading trained weights from \" + weights_path + \" ...\")\n",
        "    model2.load_weights(weights_path)\n",
        "\n",
        "    # unfreezing the base network up to a specific layer:\n",
        "    if freezeUptoLayer == \"\":\n",
        "        conv_base2.trainable = True\n",
        "        print (\"\\ntrainable layers: \",int(len(model2.trainable_weights) / 2))\n",
        "    else:\n",
        "        print(\"\\ntrainable layers before unfreezing the base network up to \" + freezeUptoLayer + \": \",int(len(model2.trainable_weights) / 2))  # weights = weights + bias = 2 pro layer\n",
        "        conv_base2.trainable = True\n",
        "        set_trainable = False\n",
        "        for layer in conv_base2.layers:\n",
        "            if layer.name == freezeUptoLayer: set_trainable = True\n",
        "            if set_trainable: layer.trainable = True\n",
        "            else: layer.trainable = False\n",
        "        print(\"trainable layers after the base network unfreezed from layer \" + freezeUptoLayer + \": \", int(len(model2.trainable_weights)/2))\n",
        "\n",
        "    print(\"\\nLEVEL2 Model after unfreezing the base network\")\n",
        "    model2.summary()\n",
        "    model2.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=learning_rate/10, decay=lr_decay), metrics=['acc'])\n",
        "    print (\"\\n### Validating ... \")\n",
        "\n",
        "    val_loss, val_acc = model2.evaluate_generator(test_generator, steps=nbrTestImages, verbose=0)\n",
        "    print('Validation Results before training unfreeze layers and trained densely connected layers:\\nValidation loss:',val_loss,\",\",'Validation accuracy:', val_acc, \"\\n\")\n",
        "\n",
        "    # Jointly training both the unfreeze layers and the added trained densely connected layers\n",
        "    callbacks_list_L2 = [ModelCheckpoint(filepath=model_path+'l2.h5', save_weights_only=False, monitor='val_acc', verbose=1, save_best_only=True),\n",
        "                      ReduceLROnPlateau(monitor='val_acc', factor=factorL2, patience=patiencel2, verbose=1),\n",
        "                      TensorBoard(log_dir=TensorBoardLogDir+'\\\\level2')]\n",
        "\n",
        "    print (\"\\n### Level2 Training ... \")\n",
        "    history = model2.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=(nbrTrainImages * classes) // 100 * batch,\n",
        "        epochs=epochsL2,\n",
        "        callbacks=callbacks_list_L2,\n",
        "        validation_data=test_generator,\n",
        "        validation_steps=nbrTestImages,\n",
        "        verbose=verbose_train)\n",
        "\n",
        "    history_val2 = [history.history]  # saving all results of the final test\n",
        "    plot(history_val2, \"LEVEL2:\", epochsL2)\n",
        "    print(\"\\n###LEVEL2 Training finished successfully ###\")\n",
        "\n",
        "\n",
        "main()\n",
        "\n",
        "# References\n",
        "# Chollet, F. (2018). Deep learning with Python. Section 5.3 - Using a pretrained convnet."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIkRdGjPwNAF",
        "colab_type": "text"
      },
      "source": [
        "#Test the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIoeUqq5wMHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from keras import models\n",
        "from keras.preprocessing import image\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "def main():\n",
        "    analysis()\n",
        "    plot_images()\n",
        "\n",
        "# Setting Parameters ##################################################################\n",
        "#model_path = 'C:\\\\Users\\\\uguru\\\\Documents\\\\ML4MIL\\\\trained_models\\\\RuTanks7000_v1_l2.h5'\n",
        "#test_path = 'C:\\\\Users\\\\uguru\\\\Documents\\\\ML4MIL\\\\datasets\\\\RuTanks7000_v1\\\\test\\\\'  #  105 Test images\n",
        "model_path = '/content/drive/My Drive/trained_models/RuTanks7000_v1__l2.h5'\n",
        "test_path = '/content/drive/My Drive/trained_models/RuTanks7000_v1/test'\n",
        "\n",
        "# image settings\n",
        "img_height, img_width = 331, 331\n",
        "\n",
        "classnames = [\"Background\", \"BMP2\", \"Buk-M1-2\", \"T14\", \"T90\", \"ZSU23\"]\n",
        "\n",
        "# test data generator\n",
        "test_datagen = image.ImageDataGenerator(rescale=1. / 255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_path,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    classes=classnames,\n",
        "    class_mode='categorical')\n",
        "\n",
        "# counting test images\n",
        "nbrTestImages = 0  # Value gets accurate after counting\n",
        "for ImagesClass in os.listdir(test_path):\n",
        "    nbrTestImages += len(os.listdir(test_path + ImagesClass))\n",
        "\n",
        "# saving test images from test generator\n",
        "test_images = []\n",
        "for i in range(nbrTestImages):\n",
        "    test_images.append(test_generator.next()[0][0])\n",
        "\n",
        "test_images_path = []\n",
        "true_labels = []\n",
        "true_labels_class = []\n",
        "\n",
        "test_images_Background = os.listdir(test_path + 'Background')\n",
        "for imagefile in test_images_Background:\n",
        "    test_images_path.append(test_path + 'Background/' + imagefile)\n",
        "    true_labels.append(0)\n",
        "    true_labels_class.append('Background')\n",
        "\n",
        "test_images_BMP2 = os.listdir(test_path + 'BMP2')\n",
        "for imagefile in test_images_BMP2:\n",
        "    test_images_path.append(test_path + 'BMP2/' + imagefile)\n",
        "    true_labels.append(1)\n",
        "    true_labels_class.append('BMP2')\n",
        "\n",
        "test_images_Buk = os.listdir(test_path + 'Buk-M1-2')\n",
        "for imagefile in test_images_Buk:\n",
        "    test_images_path.append(test_path + 'Buk-M1-2/' + imagefile)\n",
        "    true_labels.append(2)\n",
        "    true_labels_class.append('Buk-M1-2')\n",
        "\n",
        "#test_images_CivilianCar = os.listdir(test_path + 'Civilian Car')\n",
        "#for imagefile in test_images_CivilianCar:\n",
        "#    test_images_path.append(test_path + 'Civilian Car/' + imagefile)\n",
        "#    true_labels.append(3)\n",
        "#    true_labels_class.append('Civilian Car')\n",
        "\n",
        "test_images_T14 = os.listdir(test_path + 'T14')\n",
        "for imagefile in test_images_T14:\n",
        "    test_images_path.append(test_path + 'T14/' + imagefile)\n",
        "    true_labels.append(3)\n",
        "    true_labels_class.append('T14')\n",
        "\n",
        "test_images_T90 = os.listdir(test_path + 'T90')\n",
        "for imagefile in test_images_T90:\n",
        "    test_images_path.append(test_path + 'T90/' + imagefile)\n",
        "    true_labels.append(4)\n",
        "    true_labels_class.append('T90')\n",
        "\n",
        "test_images_ZSU23 = os.listdir(test_path + 'ZSU23')\n",
        "for imagefile in test_images_ZSU23:\n",
        "    test_images_path.append(test_path + 'ZSU23/' + imagefile)\n",
        "    true_labels.append(5)\n",
        "    true_labels_class.append('ZSU23')\n",
        "\n",
        "\n",
        "# Returns the compiled model identical to the previously saved one\n",
        "print(\"Loading the trained model from \" + model_path + \" ...\\n\")\n",
        "model = models.load_model(model_path)\n",
        "print(\"\\nTrained model \" + model_path + \":\")\n",
        "model.summary()\n",
        "\n",
        "# predicting labels\n",
        "pred = model.predict_generator(test_generator, nbrTestImages)\n",
        "print(\"Dataset at\", test_path, \"has\", pred.shape[0], \"images with\", pred.shape[1], \"class predictions each\")\n",
        "\n",
        "\n",
        "def analysis():\n",
        "    # testing the model\n",
        "    print(\"\\n### Testing the loaded model ... \")\n",
        "    test_loss, test_acc = model.evaluate_generator(test_generator, steps=nbrTestImages, verbose=1)\n",
        "    print('Test Results of the trained Model:\\nTest loss:', test_loss, \",\", 'test accuracy:', test_acc)\n",
        "\n",
        "    # Confusion Matrix and Classification Report\n",
        "    pred_argmax = np.argmax(pred, axis=1)\n",
        "    print('\\nConfusion Matrix')\n",
        "    print(confusion_matrix(test_generator.classes, pred_argmax))\n",
        "    print('\\nClassification Report')\n",
        "    print(classification_report(test_generator.classes, pred_argmax, target_names=classnames))\n",
        "\n",
        "    # t-SNE Visualization: t-Distributed Stochastic Neighbor Embedding (t-SNE) for visualizing high-dimensional data.\n",
        "    img_tensors = []\n",
        "    for i in range(len(test_images)):\n",
        "        pred_img = image.load_img(test_images_path[i], target_size=(331, 331))\n",
        "        img_tensor = image.img_to_array(pred_img)\n",
        "        img_tensor = np.expand_dims(img_tensor, axis=0)\n",
        "        img_tensor /= 255.\n",
        "        img_tensors.append(img_tensor)\n",
        "\n",
        "    # Last conv layer\n",
        "    layer = model.get_layer('dense_1')\n",
        "    layer_output = layer.output\n",
        "    activation_model = models.Model(input=model.input, outputs=[layer_output])\n",
        "\n",
        "    img_tensors = np.asarray(img_tensors)\n",
        "    img_tensors_array = np.concatenate((img_tensors))\n",
        "    print(\"img_tensors_array.shape=\", img_tensors_array.shape)\n",
        "\n",
        "    activations = activation_model.predict(img_tensors_array)\n",
        "    print(\"activations.shape=\", activations.shape)\n",
        "\n",
        "    # calculating tsne #https://www.datacamp.com/community/tutorials/introduction-t-sne\n",
        "    tsne = TSNE(random_state=42).fit_transform(activations)\n",
        "\n",
        "    # Visualization of the feature vectors produce by the convnet\n",
        "    num_classes = len(classnames)\n",
        "    palette = np.array(sns.color_palette(\"husl\", num_classes))\n",
        "    label_array = np.asarray(true_labels)\n",
        "\n",
        "    # create a scatter plot.\n",
        "    fig = plt.figure(figsize=(12, 12))\n",
        "    fig.patch.set_facecolor('black')\n",
        "    plt.style.use('dark_background')\n",
        "    plt.scatter(tsne[:, 0], tsne[:, 1], lw=0, s=90, c=palette[label_array])\n",
        "    plt.axis('tight')\n",
        "\n",
        "    txts = []\n",
        "    for j in range(num_classes):\n",
        "        # Position of each label at median of data points.\n",
        "        xtext, ytext = np.median(tsne[label_array == j, :], axis=0)\n",
        "        txt = plt.text(xtext, ytext, classnames[j], fontsize=28, color = 'w')\n",
        "        txts.append(txt)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_images():\n",
        "    # plotting test images with class predictions as title\n",
        "    print(\"\\nClass Predictions of test images from\", test_path)\n",
        "    for k in range(len(test_images)):\n",
        "        predictions = []\n",
        "        for l in range(len(classnames)):\n",
        "            predictions.append((np.round(pred[k][l] * 100, 2), classnames[l]))\n",
        "        predictions.sort(reverse=True)\n",
        "        print(\"Image\", k + 1, \":\", predictions)\n",
        "\n",
        "        plt.figure(figsize=(8, 8))  # width, height in inches.\n",
        "        plt.style.use('dark_background')\n",
        "        plt.suptitle(\"Class prediction of a \" + true_labels_class[k] + \" image\" + \"\\n(Test image \" + str(k + 1) + \")\", fontsize=18)\n",
        "        plt.grid(False)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        legend_string = \"Prediction: \" + \\\n",
        "                        \"\\n%.2f\" % predictions[0][0] + \"% \" + predictions[0][1] + \\\n",
        "                        \"\\n%.2f\" % predictions[1][0] + \"% \" + predictions[1][1] + \\\n",
        "                        \"\\n%.2f\" % predictions[2][0] + \"% \" + predictions[2][1] + \\\n",
        "                        \"\\n%.2f\" % predictions[3][0] + \"% \" + predictions[3][1] + \\\n",
        "                        \"\\n%.2f\" % predictions[4][0] + \"% \" + predictions[4][1] + \\\n",
        "                        \"\\n%.2f\" % predictions[5][0] + \"% \" + predictions[5][1] + \\\n",
        "                       # \"\\n%.2f\" % predictions[6][0] + \"% \" + predictions[6][1]\n",
        "        plt.text(img_width + 10, 125, legend_string, fontsize=16)\n",
        "        plt.imshow(test_images[k])\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "main()\n",
        "\n",
        "\n",
        "# References\n",
        "# Chollet, F. (2018). Deep learning with Python. Section 5.3 - Using a pretrained convnet.\n",
        "# Pathak, M (2018). Introduction to t-SNE. Retrieved from https://www.datacamp.com/community/tutorials/introduction-t-sne"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}